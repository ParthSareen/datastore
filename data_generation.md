#Overview: Tile Creation

There are a series of scripts we use to generate all of the internal and external representations of the open traffic data. Most of these data are tiles with the added dimension of time, with the one exception being the coverage map which is an all encompasing single geojson file. The creation of this data can be most easily described as a top down topology. At each stage of the topology the processing will take multiple upstream tiles and turn them into a downstream tile. Note: these processes currently rely on access to the AWS APIs for things like storage and scheduling of jobs. Currently the pipeline is setup to handle only the `AUTO` mode of transport. If we wanted to handle other modes of transport we would need to seprate the datastreams with respect to the buckets that contain the various pieces at each stage of the pipeline. Concretely, if we wanted at a motorbike dataset, we'd need to make reporter, histogram, speed tile and reference tile buckets. These buckets are accessed by the various stages described in the following sections.

##Stage 1: Histogram Generation

We have a script called `submit-work-service.py` which is run continuously on an EC2 instance. It wakes up every so often and schedules work in batch to turn files from the reporter into flatbuffer histograms. Basically it goes to the s3 bucket where the reporters are dropping data. It lists all the s3 prefixes (time tile) it sees and schedules a job for each of those in AWS Batch. Each job is for a single tile and single hour (out of all hours since the beginning of the unix epoch). As the Batch service picks up those jobs and schedules them they complete each job by running the script `work.py`. This script picks up any existing histogram for this time tile and all the reporter outputs at that s3 prefix and runs the java flatbuffer generation with those as inputs. The final output is a flatbuffer which is pushed back up to s3. These hourly histogram tiles are the basis for the next step. Histogram tile generation is not idempotent because the inputs are deleted as they are received.

##Stage 2: Speed Tile Generation

We generate speed tiles in a similar manner as the histograms. Basically we run a script called `submit-speed-tile-work-service.py` weekly that will at the end of each week submit a bunch of speed tile jobs into the AWS Batch service. Each job targets a given week of the year. The determiniation of which week gets submitted is currently set to look at what is in the speed tile data and what is in the histogram data. It figures out which week of speed tiles is the latest available and if the histogram data has coverage for the week after that it submits jobs for this week. If not it exits. You can also run this script and specify the target week direectly if new historical data needs to be folded in (ie the histograms have taken updates since the last run of speed tile generation for a given week). The speed tile job submission makes a job for each 4 degree (level 0) tile and the speed tile batch workers (which run `speed-tile-work.py`) will do the level 0 tile and the 16 level 1 tiles underneath it. Each speed tile contains a weeks worth of hourly data which it fetches from the histogram s3 bucket. When the week is finished and the speed tiles are generated they are pushed up to s3 and a job is scheduled in batch to recompute the reference tiles for the year of data upto and including the week of the newly generated speed tile. If you were making speed tile for a week we already had coverage or a week in the past the reference tile will be regenerated if it includes this new data occured within the past year (this is a TODO). Speed tile generation is idempotent, you may run it as many times as you like without affecting the veracity of the data.

##Stage 3: Reference Tile Generation

The reference tiles are generated at the end of each run of a given speed tile is finished. The AWS Batch worker uses `reference-tile-work.py` to compute the reference tile. It takes as input one or more weeks of speed tiles (for a given tile). By default we attempt to get speed tile data for the year (so 52 weeks). When finished the tile is pushed up to s3 with some meta tags specifying the range of data (min and max epoch UTC timestamp). This means that a reference tile has no set time range and that as we get updates the reference tiles will move ahead in time. These ranges will be used when coverage map generation happens.

##Stage 4: Coverage Map Generation

The coverage map takes as input all the reference tiles' metadata (the min and max time ranges) and computes a geojson file as output. This lives in the same bucket as the reference tiles.
